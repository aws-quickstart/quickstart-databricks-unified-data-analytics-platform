AWSTemplateFormatVersion: 2010-09-09
Description: >-
  This template creates Databricks workspace resources in your AWS account using the API account. The API account is required if you want to use either customer managed VPCs or customer managed keys for notebooks. For feature availability, contact your Databricks representative. (qs-1r0odiedc)

Metadata:
  cfn-lint:
    config:
      ignore_checks:
        - W3005
        - W9001
        - W9006 # temporary to get rid of warnings

  QuickStartDocumentation:
    EntrypointName: "Parameters for deploying a workspace and using an existing cross-account IAM role"
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Workspace configuration"
        Parameters:
          - AccountId
          - Username
          - Password
          - AWSRegion
          - HIPAAparm
      - Label:
          default: "Required IAM role and S3 bucket configuration"
        Parameters:
          - IAMArn
          - IAMArnLambda
          - BucketName
      - Label:
          default: "(Optional) Recommended to provide a unique deployment name for your workspace."    
        Parameters:
          - DeploymentName     
      - Label:
          default: "(Optional) Customer managed VPC configuration (premium tier required)"
        Parameters:
          - VPCID
          - SubnetIDs
          - SecurityGroupIDs
      - Label:
          default: "(Optional) Customer managed key configuration for notebooks (enterprise tier required)"
        Parameters:
          - KeyArn
          - KeyAlias
          - KeyUseCases
          - KeyReuseForClusterVolumes
      - Label:
          default: "Quick Start configuration"
        Parameters:
          - QSS3BucketName
          - QSS3KeyPrefix
    ParameterLabels:
      AccountId:
        default: Databricks account ID
      Username:
        default: Workspace account email
      Password:
        default: Workspace account password
      DeploymentName:
        default: Workspace deployment name
      AWSRegion:
        default: AWS Region of the Databricks workspace
      HIPAAparm:
        default: HIPAA tier account
      IAMArn:
        default: ARN of the existing cross-account IAM role
      IAMArnLambda:
        default: ARN of the existing IAM role with Lambda- and S3-access permissions
      BucketName:
        default: Root S3 bucket name
      VPCID:
        default: VPC ID
      SubnetIDs:
        default: Private subnet IDs
      SecurityGroupIDs:
        default: Security group IDs
      KeyArn:
        default: ARN for customer managed AWS KMS key
      KeyAlias:
        default: Alias for customer managed AWS KMS key
      KeyUseCases:
        default: Use case for the key
      KeyReuseForClusterVolumes:
        default: Encrypt cluster Amazon EBS volumes
      QSS3BucketName:
        default: Quick Start S3 bucket name
      QSS3KeyPrefix:
        default: Quick Start S3 key prefix

Outputs:
  S3BucketName:
    Description: Name of the S3 root bucket.
    Value: !Ref assetsS3Bucket
  CustomerManagedKeyId:
    Description: ID of the customer managed key object.
    Condition: IsKMSKeyProvided
    Value: !Ref createCustomerManagedKey
  CredentialsId:
    Description: Credential ID.
    Value: !Ref createCredentials
  ExternalId:
    Description: Databricks external ID.
    Value: !GetAtt createCredentials.ExternalId
  NetworkId:
    Description: Databricks network ID.
    Condition: CustomerManagedVPC
    Value: !Ref createNetworks
  StorageConfigId:
    Description: Storage configuration ID.
    Value: !Ref createStorageConfiguration
  WorkspaceURL:
    Description: URL of the workspace.
    Value: !Sub https://${createWorkspace.DeploymentName}.cloud.databricks.com'
  WorkspaceId:
    Description: Workspace ID.
    Value: !Ref createWorkspace
  WorkspaceStatus:
    Description: Status of the requested workspace.
    Value: !GetAtt createWorkspace.WorkspaceStatus
  WorkspaceStatusMessage:
    Description: Detailed status description of the requested workspace.
    Value: !GetAtt createWorkspace.WorkspaceStatusMsg
  PricingTier:
    Description: "Pricing tier of the workspace. For more information, see https://databricks.com/product/aws-pricing."
    Value: !GetAtt createWorkspace.PricingTier
  ClusterPolicyID:
    Description: Unique identifier of cluster policy.
    Value: !GetAtt createWorkspace.ClusterPolicyId

Parameters:
  AccountId:
    Description: "Account must use the E2 version of the platform. For more information, see https://docs.databricks.com/getting-started/overview.html#e2-architecture." 
    AllowedPattern: '^[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}$'
    MinLength: '36'
    Type: String
    Default: aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee
  Username:
    Description: "Account email for authenticating the REST API. Note that this value is case sensitive."
    AllowedPattern: '^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+$'
    ConstraintDescription: Must be a valid email format.
    MinLength: '8'
    Type: String
  Password:
    Description: "Account password for authenticating the REST API. The minimum length is 8 alphanumeric characters."
    MinLength: '8'
    NoEcho: 'true'
    Type: String
  DeploymentName:
    Description: "The deployment name defines part of the subdomain for the workspace. The workspace URL for web application and REST APIs is <deployment-name>.cloud.databricks.com. Accounts can have a deployment name prefix. Contact your Databricks representative to add an account deployment name prefix to your account. If your account has a non-empty deployment name prefix at workspace creation time, the workspace deployment name is updated so that it begins with the account prefix and a hyphen. If your account has a non-empty deployment name prefix and you set deployment_name to the reserved keyword EMPTY, deployment_name is the account prefix only. For more information, see https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-6-create-the-workspace"
    Type: String
  AWSRegion:
    Description: "AWS Region where the workspace is created. Note that customer managed keys to encrypt notebooks are not supported in the us-west-1 and ap-northeast-2 regions."
    AllowedValues:
       - ap-northeast-1
       - ap-northeast-2
       - ap-south-1
       - ap-southeast-1
       - ap-southeast-2
       - ca-central-1
       - eu-central-1
       - eu-west-1
       - eu-west-2
       - us-east-1
       - us-east-2
       - us-west-1
       - us-west-2
    Type: String
  HIPAAparm:
    Description: 'Entering "Yes" creates a template for creating clusters in the HIPAA account.'
    AllowedValues:
       - 'Yes'
       - 'No'
    Default: 'No'
    Type: String
  IAMArn:
    Description: "Enter an existing IAM role ARN. For more information, see https://docs.databricks.com/administration-guide/multiworkspace/iam-role.html."
    AllowedPattern: 'arn:aws:iam::\d{12}:role/.*'
    ConstraintDescription: Must be an IAM role ARN.
    MinLength: 16
    Default: 'arn:aws:iam::111111111111:role/your-role-name'
    Type: String
  IAMArnLambda:
    Description: "Enter an existing IAM role ARN with AWSLambdaBasicExecutionRole. For more information, see the deployment guide."
    AllowedPattern: 'arn:aws:iam::\d{12}:role/.*'
    ConstraintDescription: Must be an IAM role ARN.
    MinLength: 16
    Default: 'arn:aws:iam::111111111111:role/your-role-name'
    Type: String
  BucketName:
    Description: "Name of your S3 root bucket. Use only alphanumeric characters. For more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html."
    AllowedPattern: '(?=^.{3,63}$)(?!xn--)([a-z0-9](?:[a-z0-9-]*)[a-z0-9])$'
    MinLength: '3'
    MaxLength: '63'
    Type: String
    ConstraintDescription: Name of workspace root bucket. This name can include numbers, lowercase letters, uppercase letters, and hyphens, but do not start or end with a hyphen (-)."
  VPCID:
    Description: "VPC ID for creating your workspace. Only enter a value if you use the customer managed VPC feature. The format is vpc-xxxxxxxxxxxxxxxx. For more information, see https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html. If unspecified, Databricks creates a new workspace in a new VPC."
    Type: String
    Default: ''
  SecurityGroupIDs:
    Description: "Security-group names in your VPC. Only enter a value if you set VPCID. The format is sg-xxxxxxxxxxxxxxxxx. Use commas to separate multiple IDs. Databricks must have access to at least one security group but no more than five. You can reuse existing security groups. For more information, see https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html."
    Type: String
    Default: ''
  SubnetIDs:
    Description: "Enter at least two private subnet IDs. Only enter a value if you set VPCID. Subnets cannot be shared with other workspaces or non-Databricks resources. Each subnet must be private, have outbound access, and a netmask between /17 and /25. The NAT gateway must have its own subnet that routes 0.0.0.0/0 traffic to an internet gateway. For more information, see https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html."
    Type: String
    Default: ''
  KeyArn:
    Description: "AWS KMS key ARN to encrypt and decrypt workspace notebooks in the control plane. Only enter a value if you use the customer managed key for notebooks. For more information, see https://docs.databricks.com/security/keys/customer-managed-keys-notebook-aws.html."
    Type: String
    Default: ''
  KeyAlias:
    Description: "(Optional) AWS KMS key alias."
    Type: String
    Default: ''
  KeyUseCases:
    Description: "Configures customer managed encryption keys. Acceptable values are MANAGED_SERVICES, STORAGE, or BOTH. For more information, see https://docs.databricks.com/administration-guide/account-api/new-workspace.html#step-5-configure-customer-managed-keys-optional."
    Type: String
  KeyReuseForClusterVolumes:
    Description: 'Only enter a value if the use case is STORAGE or BOTH. Acceptable values are "True" and "False."'
    Type: String
  QSS3BucketName:
    Description: "S3 bucket for Quick Start assets. Use this if you want to customize the Quick Start. The bucket name can include numbers, lowercase letters, uppercase letters, and hyphens, but it cannot start or end with a hyphen (-)."
    AllowedPattern: '(?=^.{3,63}$)(?!xn--)([a-z0-9](?:[a-z0-9-]*)[a-z0-9])$'
    Default: aws-quickstart
    Type: String
    ConstraintDescription: 'Quick Start bucket name can include numbers, lowercase letters, uppercase letters, and hyphens, but it cannot start or end with a hyphen (-).'
  QSS3KeyPrefix:
    Description: "S3 key prefix to simulate a directory for your Quick Start assets. Use this if you want to customize the Quick Start. The prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slashes (/). For more information, see https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html."
    ConstraintDescription: 'The Quick Start key prefix can include numbers, lowercase letters, uppercase letters, hyphens (-), and forward slashes (/).'
    AllowedPattern: '^[0-9a-zA-Z-/]*$'
    Default: quickstart-databricks-unified-data-analytics-platform/
    Type: String


Conditions:
# Set condition when VPC ID is provided by the user
  CustomerManagedVPC: !Not [!Equals [!Ref VPCID, '']]
# Set condition when AWS KMS key ID is provided by the user.
  IsKMSKeyProvided: !Not [!Equals [!Ref KeyArn, '']]
# Test for Cluster Volume parameter is set
  ClusterVolumeSet: !Equals [!Ref KeyReuseForClusterVolumes, 'True'] 
# Test for the OPTIONAL deployment name
  IsDeploymentNameSet: !Not [!Equals [!Ref DeploymentName, '']]

Rules:
# 1. Validate the selected Region from drop-down matches the Region from the Console
  RunningTemplateFromDifferentRegionThanDropDown:
    Assertions:
    - Assert: !Equals [!Ref AWSRegion, !Ref 'AWS::Region']
      AssertDescription: Region from the AWS Management Console must match the selected Region from the drop-down menu.

# 2. HIPAA supported regions, if HIPAA flaf is set
  SupportedHipaaRegions:
    RuleCondition: !Equals [!Ref HIPAAparm, 'Yes']
    Assertions:
    - Assert: !Contains [['us-east-1', 'us-east-2', 'ca-central-1'], !Ref AWSRegion]
      AssertDescription: 'Creates a workspace for only HIPAA tier accounts in the us-east-1, us-east-2, and ca-central-1 Regions.'

# 3. Optional section. Ensure that the SubnetIds and SecurityGroupIds are provided, if the user provides a customer managed VPC ID
  CustomerManagedVPC:
    RuleCondition: !Not [!Equals [!Ref VPCID, '']]
    Assertions:
    - Assert: !Not [!Equals ['', !Ref SubnetIDs]]
      AssertDescription: SubnetIDs is required when VPCID is provided.
    - Assert: !Not [!Equals ['', !Ref SecurityGroupIDs]]
      AssertDescription: SecurityGroupIDs is required when VPCID is provided.

# 4. Optional Section. Ensure the KeyARN is provided when a use case is specified.
  KeyUseCases1:
    RuleCondition: !Not [!Equals [!Ref KeyArn, '']]
    Assertions:
    - Assert: !Contains [['MANAGED_SERVICES', 'STORAGE', 'BOTH'],!Ref KeyUseCases]
      AssertDescription: Acceptable values are MANAGED_SERVICES, STORAGE, or BOTH when you provide a role ARN.
    
  KeyUseCases2:
    RuleCondition: !Or
      - !Equals [!Ref KeyUseCases, 'STORAGE']
      - !Equals [!Ref KeyUseCases, 'BOTH']
    Assertions:
    - Assert: !Contains [['True', 'False'], !Ref KeyReuseForClusterVolumes]
      AssertDescription: 'Acceptable values are "True" and "False" when the use case is either STORAGE or BOTH.'
 
  KeyUseCases3:
    RuleCondition: !Equals [!Ref KeyUseCases, 'MANAGED_SERVICES']
    Assertions: 
    - Assert: !Equals [!Ref KeyReuseForClusterVolumes, '']
      AssertDescription: Value must be null if MANAGED_SERVICES is provided. 

# 5. Assertion rule to prevent changing the QuickStart Bucket name and Prefix parameter
# ***********************************************************************************************************************
# This rule must be COMMENTED if it is intended to clone the git repo to make modifications prior to promote the changes
# ***********************************************************************************************************************
  AWSQuickStartGitParametersSettings:
    Assertions:
    - Assert: !Equals ['aws-quickstart', !Ref QSS3BucketName]
      AssertDescription: QSS3BucketName must be set to aws-quickstart.
    - Assert: !Equals ['quickstart-databricks-unified-data-analytics-platform/', !Ref QSS3KeyPrefix]
      AssertDescription: "QSS3KeyPrefix must be set to quickstart-databricks-unified-data-analytics-platform/."
       

Resources: 
  # S3 root bucket requirements
  assetsS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls       : true
        BlockPublicPolicy     : true
        IgnorePublicAcls      : true
        RestrictPublicBuckets : true
  bucketPolicy:
    Type: 'AWS::S3::BucketPolicy'
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - EIAMAccountIDInPrincipal
          ignore_reasons:
            EIAMAccountIDInPrincipal: "Hardcoded account ID needed for configuration: https://docs.databricks.com/administration-guide/account-settings/aws-accounts.html#step-2-create-a-cross-account-role-and-an-access-policy"
    Properties:
      PolicyDocument:
        Id: MyPolicy
        Version: 2012-10-17
        Statement:
          - Sid: Grant Databricks Access
            Effect: Allow
            Principal:
              AWS: arn:aws:iam::414351767826:root
            Action:
              - 's3:GetObject'
              - 's3:GetObjectVersion'
              - 's3:PutObject'
              - 's3:DeleteObject'
              - 's3:ListBucket'
              - 's3:GetBucketLocation'
            Resource:
              - !Sub 'arn:${AWS::Partition}:s3:::${assetsS3Bucket}/*'
              - !Sub 'arn:${AWS::Partition}:s3:::${assetsS3Bucket}'
      Bucket: !Ref assetsS3Bucket

  # Databricks API for configuring notebook encryption with a customer managed KMS, if provided
  createCustomerManagedKey:
    Condition: IsKMSKeyProvided
    DependsOn: updateCustomManagedKeys
    Type: Custom::CreateCustomerManagedKey
    Properties:
      ServiceToken: !GetAtt databricksApiFunction.Arn
      action: CREATE_CUSTOMER_MANAGED_KEY
      accountId: !Ref AccountId
      username: !Ref Username
      password: !Ref Password
      key_arn: !Ref KeyArn
      key_alias: !Ref KeyAlias
      use_cases: !Ref KeyUseCases
      reuse_key_for_cluster_volumes: !Ref KeyReuseForClusterVolumes
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for workspace credentials
  createCredentials:
    Type: Custom::CreateCredentials
    Properties:
      ServiceToken: !GetAtt databricksApiFunction.Arn
      action: CREATE_CREDENTIALS
      accountId: !Ref AccountId
      username: !Ref Username
      password: !Ref Password
      credentials_name: !If [IsDeploymentNameSet, !Sub '${DeploymentName}-credentials', !Sub '${AWS::StackName}-credentials']
      role_arn: !Ref IAMArn
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for workspace storage configuration
  createStorageConfiguration:
    Type: Custom::CreateStorageConfigurations
    Properties:
      ServiceToken: !GetAtt databricksApiFunction.Arn
      action: CREATE_STORAGE_CONFIGURATIONS
      accountId: !Ref AccountId
      username: !Ref Username
      password: !Ref Password
      storage_config_name: !If [IsDeploymentNameSet, !Sub '${DeploymentName}-storage', !Sub '${AWS::StackName}-storage']
      s3bucket_name: !Ref assetsS3Bucket
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for network configuration
  createNetworks:
    DependsOn: createStorageConfiguration
    Condition: CustomerManagedVPC
    Type: Custom::createNetworks
    Properties:
      ServiceToken: !GetAtt databricksApiFunction.Arn
      action: CREATE_NETWORKS
      accountId: !Ref AccountId
      username: !Ref Username
      password: !Ref Password
      network_name: !If [IsDeploymentNameSet, !Sub '${DeploymentName}-network', !Sub '${AWS::StackName}-network']
      vpc_id: !Ref VPCID
      subnet_ids: !Ref SubnetIDs
      security_group_ids: !Ref SecurityGroupIDs
      user_agent: 'databricks-CloudFormation-provider'

  # Databricks API for workspace creation
  createWorkspace:
    Type: Custom::CreateWorkspace
    Properties:
      ServiceToken: !GetAtt databricksApiFunction.Arn
      action: CREATE_WORKSPACES
      accountId: !Ref AccountId
      username: !Ref Username
      password: !Ref Password
      workspace_name: !If [IsDeploymentNameSet, !Sub '${DeploymentName}-workspace', !Sub '${AWS::StackName}-workspace']
      deployment_name: !Ref DeploymentName
      aws_region: !Ref AWSRegion
      credentials_id: !Ref createCredentials
      storage_config_id: !Ref createStorageConfiguration
      network_id: !If [CustomerManagedVPC, !Ref createNetworks, !Ref AWS::NoValue]
      customer_managed_key_id: !If [IsKMSKeyProvided, !Ref createCustomerManagedKey, !Ref AWS::NoValue]
      hipaa_parm: !Ref HIPAAparm
      user_agent: 'databricks-CloudFormation-provider'

  # Customer managed Keys - Update policy for S3 and EBS volumes 
  updateCustomManagedKeys:
    Condition: IsKMSKeyProvided
    DependsOn: updateKMSkeysFunction
    Type: Custom::updateCustomManagedKeys
    Properties:
      ServiceToken: !GetAtt updateKMSkeysFunction.Arn
      key_id: !Ref KeyArn
      arn_credentials: !If [ClusterVolumeSet, !Ref 'IAMArn', '']
      use_cases: !Ref KeyUseCases
      reuse_key_for_cluster_volumes: !Ref KeyReuseForClusterVolumes

  # Databricks main Lambda for all E2 objects and workspace creation
  databricksApiFunction:
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Databricks account API.
      Handler: rest_client.handler
      Runtime: python3.8
      Role: !Ref IAMArnLambda
      Timeout: 900
      Code:
        S3Bucket: !Ref LambdaZipsBucket
        S3Key: !Sub ${QSS3KeyPrefix}functions/packages/lambda.zip
  
  # Databricks CMK lambda
  updateKMSkeysFunction:
    Condition: IsKMSKeyProvided
    DependsOn: CopyZips
    Type: AWS::Lambda::Function
    Properties:
      Description: Update CMK policy document for storage.
      Handler: update_custommanaged_cmk_policy.handler
      Runtime: python3.8
      Role: !Ref IAMArnLambda
      Timeout: 60
      Code:
        S3Bucket: !Ref LambdaZipsBucket
        S3Key: !Sub ${QSS3KeyPrefix}functions/packages/lambda.zip

  # Resources to stage lambda.zip file
  LambdaZipsBucket:
    Type: AWS::S3::Bucket
  CopyZips:
    Type: Custom::CopyZips
    Properties:
      ServiceToken: !GetAtt CopyZipsFunction.Arn
      DestBucket: !Ref LambdaZipsBucket
      SourceBucket: !Ref QSS3BucketName
      Prefix: !Ref QSS3KeyPrefix
      Objects:
        - functions/packages/lambda.zip

  CopyZipsFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Copies objects from an S3 bucket to another destination.
      Handler: index.handler
      Runtime: python3.7
      Role: !Ref 'IAMArnLambda'
      Timeout: 240
      Code:
        ZipFile: |
          import json
          import logging
          import threading
          import boto3
          import cfnresponse
          def copy_objects(source_bucket, dest_bucket, prefix, objects):
              s3 = boto3.client('s3')
              for o in objects:
                  key = prefix + o
                  copy_source = {
                      'Bucket': source_bucket,
                      'Key': key
                  }
                  print('copy_source: %s' % copy_source)
                  print('dest_bucket = %s'%dest_bucket)
                  print('key = %s' %key)
                  s3.copy_object(CopySource=copy_source, Bucket=dest_bucket,
                        Key=key)
          def delete_objects(bucket, prefix, objects):
              s3 = boto3.client('s3')
              objects = {'Objects': [{'Key': prefix + o} for o in objects]}
              s3.delete_objects(Bucket=bucket, Delete=objects)
          def timeout(event, context):
              logging.error('Execution is about to time out, sending failure response to CloudFormation')
              cfnresponse.send(event, context, cfnresponse.FAILED, {}, None)
          def handler(event, context):
              # make sure we send a failure to CloudFormation if the function
              # is going to timeout
              timer = threading.Timer((context.get_remaining_time_in_millis()
                        / 1000.00) - 0.5, timeout, args=[event, context])
              timer.start()
              print('Received event: %s' % json.dumps(event))
              status = cfnresponse.SUCCESS
              try:
                  source_bucket = event['ResourceProperties']['SourceBucket']
                  dest_bucket = event['ResourceProperties']['DestBucket']
                  prefix = event['ResourceProperties']['Prefix']
                  objects = event['ResourceProperties']['Objects']
                  if event['RequestType'] == 'Delete':
                      delete_objects(dest_bucket, prefix, objects)
                  else:
                      copy_objects(source_bucket, dest_bucket, prefix, objects)
              except Exception as e:
                  logging.error('Exception: %s' % e, exc_info=True)
                  status = cfnresponse.FAILED
              finally:
                  timer.cancel()
                  cfnresponse.send(event, context, status, {}, None)
